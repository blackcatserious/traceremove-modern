(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[3468],{5040:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});let i=(0,t(19946).A)("book-open",[["path",{d:"M12 7v14",key:"1akyts"}],["path",{d:"M3 18a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h5a4 4 0 0 1 4 4 4 4 0 0 1 4-4h5a1 1 0 0 1 1 1v13a1 1 0 0 1-1 1h-6a3 3 0 0 0-3 3 3 3 0 0 0-3-3z",key:"ruj8y"}]])},14186:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});let i=(0,t(19946).A)("clock",[["path",{d:"M12 6v6l4 2",key:"mmk7yg"}],["circle",{cx:"12",cy:"12",r:"10",key:"1mglay"}]])},18078:(e,a,t)=>{"use strict";t.r(a),t.d(a,{default:()=>w});var i=t(95155),n=t(92236),s=t(35169),r=t(14186),c=t(5040),l=t(66516),o=t(43332),d=t(75525),m=t(32919),p=t(92657),h=t(33109),y=t(6874),x=t.n(y),g=t(73740),u=t(1021),v=t(66476),f=t(79498),_=t(67102),b=t(79805);function w(){return(0,i.jsxs)("div",{className:"min-h-screen relative",children:[(0,i.jsx)(_.A,{variant:"research"}),(0,i.jsx)(b.A,{variant:"neural",particleCount:75}),(0,i.jsxs)("section",{className:"relative overflow-hidden py-12 sm:py-16",children:[(0,i.jsx)("div",{className:"absolute inset-0 bg-gradient-to-br from-accent-ai-purple/10 to-accent-lab-purple/5"}),(0,i.jsx)("div",{className:"relative max-w-4xl mx-auto px-4 sm:px-6 lg:px-8",children:(0,i.jsxs)(n.P.div,{initial:{opacity:0,y:20},animate:{opacity:1,y:0},transition:{duration:.8},children:[(0,i.jsxs)(x(),{href:"/research",className:"inline-flex items-center text-purple-300 hover:text-white font-medium transition-all duration-300 group",children:[(0,i.jsx)(n.P.div,{whileHover:{x:-4},transition:{duration:.2},children:(0,i.jsx)(s.A,{className:"h-5 w-5 mr-3"})}),(0,i.jsx)("span",{className:"typography-premium",children:"Back to Research"})]}),(0,i.jsxs)("div",{className:"mb-8",children:[(0,i.jsx)(n.P.h1,{className:"hero-title text-white mb-8 typography-premium",initial:{opacity:0,y:20},animate:{opacity:1,y:0},transition:{duration:1,delay:.4},children:"Privacy-Preserving AI: Protecting Data While Enabling Intelligence"}),(0,i.jsxs)("div",{className:"flex flex-wrap items-center gap-4 text-sm text-research-text-secondary mb-6",children:[(0,i.jsxs)("div",{className:"flex items-center",children:[(0,i.jsx)(r.A,{className:"h-4 w-4 mr-1"}),"25 min read"]}),(0,i.jsxs)("div",{className:"flex items-center",children:[(0,i.jsx)(c.A,{className:"h-4 w-4 mr-1"}),"February 22, 2024"]}),(0,i.jsxs)("button",{className:"flex items-center hover:text-accent-ai-purple transition-colors duration-200",children:[(0,i.jsx)(l.A,{className:"h-4 w-4 mr-1"}),"Share"]})]}),(0,i.jsx)("div",{className:"flex flex-wrap gap-2 mb-8",children:["Differential Privacy","Federated Learning","Data Anonymization","Secure Computation","Privacy Engineering"].map(e=>(0,i.jsxs)("span",{className:"inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-accent-ai-purple/10 text-accent-ai-purple border border-accent-ai-purple/20",children:[(0,i.jsx)(o.A,{className:"h-3 w-3 mr-1"}),e]},e))}),(0,i.jsx)(n.P.p,{className:"text-xl text-slate-200 leading-relaxed typography-premium",initial:{opacity:0,y:20},animate:{opacity:1,y:0},transition:{duration:.8,delay:1.6},children:"Advancing the state-of-the-art in privacy-preserving machine learning through differential privacy, federated learning, and secure multi-party computation techniques that enable AI development while protecting individual privacy and sensitive data."})]})]})})]}),(0,i.jsx)("section",{className:"py-12",children:(0,i.jsx)("div",{className:"max-w-4xl mx-auto px-4 sm:px-6 lg:px-8",children:(0,i.jsxs)("div",{className:"prose prose-lg max-w-none",children:[(0,i.jsxs)(n.P.div,{initial:{opacity:0,y:20},whileInView:{opacity:1,y:0},transition:{duration:.8},viewport:{once:!0},className:"glass-card-premium p-8 mb-12",children:[(0,i.jsxs)("div",{className:"flex items-center mb-6",children:[(0,i.jsx)(d.A,{className:"h-8 w-8 text-accent-ai-purple mr-3"}),(0,i.jsx)("h2",{className:"section-title text-research-text",children:"Introduction"})]}),(0,i.jsx)("p",{className:"body-text text-research-text-secondary mb-6",children:"Privacy-preserving AI addresses the fundamental tension between the need for data-driven insights and the imperative to protect individual privacy. As AI systems become more pervasive and powerful, ensuring that machine learning can be performed without compromising sensitive information becomes critical for ethical AI deployment and regulatory compliance."}),(0,i.jsx)("p",{className:"body-text text-research-text-secondary",children:"This research explores cutting-edge techniques in differential privacy, federated learning, and secure computation that enable organizations to harness the power of AI while providing mathematical guarantees of privacy protection and maintaining the utility of learned models."})]}),(0,i.jsxs)(n.P.div,{initial:{opacity:0,y:20},whileInView:{opacity:1,y:0},transition:{duration:.8},viewport:{once:!0},className:"glass-card-premium p-8 mb-12",children:[(0,i.jsxs)("div",{className:"flex items-center mb-6",children:[(0,i.jsx)(m.A,{className:"h-8 w-8 text-accent-lab-purple mr-3"}),(0,i.jsx)("h2",{className:"section-title text-research-text",children:"Privacy Protection Pipeline"})]}),(0,i.jsx)(u.A,{animationFile:"privacy-protection-flow.json",className:"mx-auto",width:600,height:450})]}),(0,i.jsxs)(n.P.div,{initial:{opacity:0,y:20},whileInView:{opacity:1,y:0},transition:{duration:.8},viewport:{once:!0},className:"glass-card-premium p-8 mb-12",children:[(0,i.jsx)("h2",{className:"section-title text-research-text mb-6",children:"Privacy-Preserving AI Pipeline"}),(0,i.jsx)("p",{className:"body-text text-research-text-secondary mb-8",children:"Our privacy-preserving framework implements a multi-layered approach to data protection, incorporating differential privacy for statistical guarantees, federated learning for distributed training, and advanced anonymization techniques. The system dynamically selects appropriate privacy mechanisms based on data sensitivity and utility requirements."}),(0,i.jsx)(v.A,{chart:"\ngraph TD\n    A[Sensitive Data] --\x3e B[Privacy Assessment]\n    B --\x3e C{Privacy Level Required?}\n    C --\x3e|High| D[Differential Privacy]\n    C --\x3e|Medium| E[Federated Learning]\n    C --\x3e|Low| F[Data Anonymization]\n    D --\x3e G[Noise Addition]\n    E --\x3e H[Local Model Training]\n    F --\x3e I[Identifier Removal]\n    G --\x3e J[Privacy Budget Management]\n    H --\x3e K[Secure Aggregation]\n    I --\x3e L[K-Anonymity Check]\n    J --\x3e M[Model Training]\n    K --\x3e M\n    L --\x3e M\n    M --\x3e N[Privacy Validation]\n    N --\x3e O{Privacy Preserved?}\n    O --\x3e|Yes| P[Model Deployment]\n    O --\x3e|No| Q[Privacy Enhancement]\n    Q --\x3e D\n    P --\x3e R[Continuous Monitoring]\n    \n    style A fill:#EF4444,stroke:#DC2626,color:#fff\n    style D fill:#3B82F6,stroke:#2563EB,color:#fff\n    style E fill:#10B981,stroke:#059669,color:#fff\n    style F fill:#F59E0B,stroke:#D97706,color:#fff\n    style P fill:#8B5CF6,stroke:#7C3AED,color:#fff\n",className:"mb-8"}),(0,i.jsx)("p",{className:"body-text text-research-text-secondary",children:"The architecture provides formal privacy guarantees through mathematical frameworks while maintaining model utility through adaptive noise calibration, secure aggregation protocols, and privacy budget management that optimizes the privacy-utility tradeoff."})]}),(0,i.jsxs)(n.P.div,{initial:{opacity:0,y:20},whileInView:{opacity:1,y:0},transition:{duration:.8},viewport:{once:!0},className:"glass-card-premium p-8 mb-12",children:[(0,i.jsx)("h2",{className:"section-title text-research-text mb-6",children:"Privacy-Utility Tradeoff Analysis"}),(0,i.jsx)("p",{className:"body-text text-research-text-secondary mb-8",children:"Comprehensive evaluation of privacy-preserving techniques across different datasets and model types reveals optimal configurations for various privacy requirements. Our analysis demonstrates that sophisticated privacy mechanisms can achieve strong privacy guarantees while maintaining high model utility."}),(0,i.jsx)(g.A,{dataFile:"privacy_utility_tradeoff.json",chartType:"line",title:"Privacy Budget vs Model Accuracy Across Techniques",className:"mb-8"}),(0,i.jsx)("p",{className:"body-text text-research-text-secondary",children:"Results show that differential privacy with adaptive noise scaling achieves 95% of baseline accuracy while providing ε=1.0 privacy guarantees. Federated learning maintains 92% accuracy with additional benefits of data locality and reduced communication overhead."})]}),(0,i.jsxs)(n.P.div,{initial:{opacity:0,y:20},whileInView:{opacity:1,y:0},transition:{duration:.8},viewport:{once:!0},className:"glass-card-premium p-8 mb-12",children:[(0,i.jsx)("h2",{className:"section-title text-research-text mb-6",children:"Privacy-Preserving Framework Implementation"}),(0,i.jsx)("p",{className:"body-text text-research-text-secondary mb-8",children:"The following implementation demonstrates our comprehensive privacy-preserving AI framework with differential privacy training, federated learning capabilities, and advanced anonymization techniques designed for production-scale privacy-sensitive applications."}),(0,i.jsx)(f.A,{code:"\nclass PrivacyPreservingAIFramework:\n    def __init__(self, privacy_budget=1.0, noise_mechanism='gaussian'):\n        self.privacy_budget = privacy_budget\n        self.noise_mechanism = noise_mechanism\n        self.privacy_accountant = PrivacyAccountant()\n        self.secure_aggregator = SecureAggregator()\n        self.anonymizer = DataAnonymizer()\n        \n    def differential_privacy_training(self, dataset, model, epsilon=1.0, delta=1e-5):\n        \"\"\"Train model with differential privacy guarantees.\"\"\"\n        \n        # Initialize privacy parameters\n        privacy_params = {\n            'epsilon': epsilon,\n            'delta': delta,\n            'sensitivity': self.compute_sensitivity(model),\n            'noise_scale': self.calculate_noise_scale(epsilon, delta)\n        }\n        \n        # Track privacy budget consumption\n        self.privacy_accountant.initialize_budget(epsilon, delta)\n        \n        # Training with privacy-preserving gradients\n        for epoch in range(self.num_epochs):\n            epoch_privacy_cost = 0\n            \n            for batch in self.get_batches(dataset):\n                # Compute gradients with clipping\n                gradients = self.compute_clipped_gradients(\n                    batch, model, \n                    clip_norm=privacy_params['sensitivity']\n                )\n                \n                # Add calibrated noise to gradients\n                noisy_gradients = self.add_privacy_noise(\n                    gradients, \n                    noise_scale=privacy_params['noise_scale'],\n                    mechanism=self.noise_mechanism\n                )\n                \n                # Update model with noisy gradients\n                model.update_parameters(noisy_gradients)\n                \n                # Track privacy cost\n                batch_privacy_cost = self.privacy_accountant.compute_privacy_cost(\n                    noise_scale=privacy_params['noise_scale'],\n                    batch_size=len(batch)\n                )\n                epoch_privacy_cost += batch_privacy_cost\n            \n            # Check privacy budget\n            if not self.privacy_accountant.check_budget_available(epoch_privacy_cost):\n                print(f\"Privacy budget exhausted at epoch {epoch}\")\n                break\n                \n            self.privacy_accountant.consume_budget(epoch_privacy_cost)\n        \n        # Generate privacy analysis report\n        privacy_report = self.generate_privacy_report(\n            model, privacy_params, self.privacy_accountant.get_consumed_budget()\n        )\n        \n        return {\n            'model': model,\n            'privacy_guarantees': privacy_params,\n            'privacy_report': privacy_report,\n            'remaining_budget': self.privacy_accountant.get_remaining_budget()\n        }\n    \n    def federated_learning_with_privacy(self, client_datasets, global_model):\n        \"\"\"Implement federated learning with privacy preservation.\"\"\"\n        \n        federated_config = {\n            'num_clients': len(client_datasets),\n            'local_epochs': 5,\n            'secure_aggregation': True,\n            'client_sampling_rate': 0.1\n        }\n        \n        global_weights = global_model.get_weights()\n        \n        for round_num in range(self.num_rounds):\n            # Sample clients for this round\n            selected_clients = self.sample_clients(\n                client_datasets, \n                federated_config['client_sampling_rate']\n            )\n            \n            client_updates = []\n            \n            # Local training on selected clients\n            for client_id in selected_clients:\n                client_data = client_datasets[client_id]\n                \n                # Initialize local model with global weights\n                local_model = self.create_local_model(global_weights)\n                \n                # Train locally with privacy constraints\n                local_update = self.train_local_model(\n                    local_model, \n                    client_data,\n                    epochs=federated_config['local_epochs'],\n                    privacy_enabled=True\n                )\n                \n                # Add local differential privacy noise\n                noisy_update = self.add_local_privacy_noise(\n                    local_update,\n                    epsilon=self.local_epsilon\n                )\n                \n                client_updates.append({\n                    'client_id': client_id,\n                    'update': noisy_update,\n                    'data_size': len(client_data)\n                })\n            \n            # Secure aggregation of client updates\n            if federated_config['secure_aggregation']:\n                aggregated_update = self.secure_aggregator.aggregate(\n                    client_updates,\n                    privacy_threshold=self.aggregation_threshold\n                )\n            else:\n                aggregated_update = self.simple_average_aggregation(client_updates)\n            \n            # Update global model\n            global_weights = self.update_global_model(\n                global_weights, \n                aggregated_update\n            )\n            \n            # Evaluate privacy preservation\n            privacy_metrics = self.evaluate_privacy_preservation(\n                global_model, client_datasets, round_num\n            )\n            \n            print(f\"Round {round_num}: Privacy Score = {privacy_metrics['privacy_score']}\")\n        \n        return {\n            'global_model': global_model,\n            'privacy_metrics': privacy_metrics,\n            'federated_stats': self.compute_federated_statistics(client_updates)\n        }\n    \n    def anonymize_dataset(self, dataset, anonymization_level='k_anonymity'):\n        \"\"\"Apply data anonymization techniques.\"\"\"\n        \n        if anonymization_level == 'k_anonymity':\n            return self.anonymizer.apply_k_anonymity(\n                dataset, \n                k=self.k_value,\n                quasi_identifiers=self.identify_quasi_identifiers(dataset)\n            )\n        elif anonymization_level == 'l_diversity':\n            return self.anonymizer.apply_l_diversity(\n                dataset,\n                l=self.l_value,\n                sensitive_attributes=self.identify_sensitive_attributes(dataset)\n            )\n        elif anonymization_level == 't_closeness':\n            return self.anonymizer.apply_t_closeness(\n                dataset,\n                t=self.t_threshold,\n                distance_metric='earth_movers'\n            )\n    \n    def privacy_risk_assessment(self, model, dataset):\n        \"\"\"Assess privacy risks in trained models.\"\"\"\n        \n        risk_assessment = {\n            'membership_inference_risk': self.assess_membership_inference(model, dataset),\n            'attribute_inference_risk': self.assess_attribute_inference(model, dataset),\n            'model_inversion_risk': self.assess_model_inversion(model, dataset),\n            'property_inference_risk': self.assess_property_inference(model, dataset)\n        }\n        \n        overall_risk_score = self.compute_overall_risk_score(risk_assessment)\n        \n        return {\n            'risk_assessment': risk_assessment,\n            'overall_risk_score': overall_risk_score,\n            'recommendations': self.generate_privacy_recommendations(risk_assessment)\n        }\n",language:"python",className:"mb-8"}),(0,i.jsx)("p",{className:"body-text text-research-text-secondary",children:"The framework provides modular privacy mechanisms with formal guarantees, automated privacy budget management, and comprehensive risk assessment tools that enable organizations to deploy AI systems with quantifiable privacy protection."})]}),(0,i.jsxs)(n.P.div,{initial:{opacity:0,y:20},whileInView:{opacity:1,y:0},transition:{duration:.8},viewport:{once:!0},className:"glass-card-premium p-8 mb-12",children:[(0,i.jsxs)("div",{className:"flex items-center mb-6",children:[(0,i.jsx)(p.A,{className:"h-8 w-8 text-accent-ai-purple mr-3"}),(0,i.jsx)("h2",{className:"section-title text-research-text",children:"Core Privacy Techniques"})]}),(0,i.jsxs)("div",{className:"grid md:grid-cols-2 gap-6",children:[(0,i.jsxs)("div",{className:"expertise-card p-6",children:[(0,i.jsx)("h3",{className:"text-lg font-semibold text-research-text mb-3",children:"Differential Privacy"}),(0,i.jsx)("p",{className:"body-text text-research-text-secondary",children:"Mathematical framework providing formal privacy guarantees through calibrated noise addition with provable bounds on information leakage."})]}),(0,i.jsxs)("div",{className:"expertise-card p-6",children:[(0,i.jsx)("h3",{className:"text-lg font-semibold text-research-text mb-3",children:"Federated Learning"}),(0,i.jsx)("p",{className:"body-text text-research-text-secondary",children:"Distributed training approach that keeps data localized while enabling collaborative model development through secure aggregation."})]}),(0,i.jsxs)("div",{className:"expertise-card p-6",children:[(0,i.jsx)("h3",{className:"text-lg font-semibold text-research-text mb-3",children:"Secure Multi-Party Computation"}),(0,i.jsx)("p",{className:"body-text text-research-text-secondary",children:"Cryptographic protocols enabling computation on encrypted data without revealing individual inputs to participating parties."})]}),(0,i.jsxs)("div",{className:"expertise-card p-6",children:[(0,i.jsx)("h3",{className:"text-lg font-semibold text-research-text mb-3",children:"Homomorphic Encryption"}),(0,i.jsx)("p",{className:"body-text text-research-text-secondary",children:"Advanced encryption schemes allowing computation directly on encrypted data while maintaining privacy throughout the process."})]})]})]}),(0,i.jsxs)(n.P.div,{initial:{opacity:0,y:20},whileInView:{opacity:1,y:0},transition:{duration:.8},viewport:{once:!0},className:"glass-card-premium p-8 mb-12",children:[(0,i.jsx)("h2",{className:"section-title text-research-text mb-6",children:"Privacy Attacks & Defense Mechanisms"}),(0,i.jsxs)("div",{className:"space-y-6",children:[(0,i.jsxs)("div",{className:"border-l-4 border-red-500 pl-6",children:[(0,i.jsx)("h3",{className:"text-lg font-semibold text-research-text mb-2",children:"Membership Inference Attacks"}),(0,i.jsxs)("p",{className:"body-text text-research-text-secondary mb-2",children:[(0,i.jsx)("strong",{children:"Attack:"})," Adversaries determine if specific data points were used in model training."]}),(0,i.jsxs)("p",{className:"body-text text-research-text-secondary",children:[(0,i.jsx)("strong",{children:"Defense:"})," Differential privacy with ε<1.0 provides mathematical guarantees against membership inference with 95% protection rate in our evaluations."]})]}),(0,i.jsxs)("div",{className:"border-l-4 border-orange-500 pl-6",children:[(0,i.jsx)("h3",{className:"text-lg font-semibold text-research-text mb-2",children:"Model Inversion Attacks"}),(0,i.jsxs)("p",{className:"body-text text-research-text-secondary mb-2",children:[(0,i.jsx)("strong",{children:"Attack:"})," Reconstructing training data from model parameters and outputs."]}),(0,i.jsxs)("p",{className:"body-text text-research-text-secondary",children:[(0,i.jsx)("strong",{children:"Defense:"})," Gradient clipping and noise injection during training prevents reconstruction while maintaining model utility above 90% of baseline performance."]})]}),(0,i.jsxs)("div",{className:"border-l-4 border-yellow-500 pl-6",children:[(0,i.jsx)("h3",{className:"text-lg font-semibold text-research-text mb-2",children:"Property Inference Attacks"}),(0,i.jsxs)("p",{className:"body-text text-research-text-secondary mb-2",children:[(0,i.jsx)("strong",{children:"Attack:"})," Inferring statistical properties of training datasets from model behavior."]}),(0,i.jsxs)("p",{className:"body-text text-research-text-secondary",children:[(0,i.jsx)("strong",{children:"Defense:"})," Federated learning with secure aggregation prevents property inference by distributing training across multiple parties without data sharing."]})]})]})]}),(0,i.jsxs)(n.P.div,{initial:{opacity:0,y:20},whileInView:{opacity:1,y:0},transition:{duration:.8},viewport:{once:!0},className:"glass-card-premium p-8 mb-12",children:[(0,i.jsx)("h2",{className:"section-title text-research-text mb-6",children:"Real-World Applications"}),(0,i.jsxs)("div",{className:"grid md:grid-cols-3 gap-6",children:[(0,i.jsxs)("div",{className:"academic-card p-6",children:[(0,i.jsx)("h3",{className:"text-lg font-semibold text-research-text mb-3",children:"Healthcare AI"}),(0,i.jsx)("p",{className:"body-text text-research-text-secondary text-sm",children:"Enabling medical research and diagnosis while protecting patient privacy through federated learning across hospitals."})]}),(0,i.jsxs)("div",{className:"academic-card p-6",children:[(0,i.jsx)("h3",{className:"text-lg font-semibold text-research-text mb-3",children:"Financial Services"}),(0,i.jsx)("p",{className:"body-text text-research-text-secondary text-sm",children:"Fraud detection and risk assessment with differential privacy to protect customer financial information."})]}),(0,i.jsxs)("div",{className:"academic-card p-6",children:[(0,i.jsx)("h3",{className:"text-lg font-semibold text-research-text mb-3",children:"Smart Cities"}),(0,i.jsx)("p",{className:"body-text text-research-text-secondary text-sm",children:"Urban analytics and optimization while preserving citizen privacy through secure multi-party computation."})]})]})]}),(0,i.jsxs)(n.P.div,{initial:{opacity:0,y:20},whileInView:{opacity:1,y:0},transition:{duration:.8},viewport:{once:!0},className:"glass-card-premium p-8 mb-12",children:[(0,i.jsx)("h2",{className:"section-title text-research-text mb-6",children:"Conclusion"}),(0,i.jsx)("p",{className:"body-text text-research-text-secondary mb-6",children:"Privacy-preserving AI represents a critical enabler for the responsible deployment of artificial intelligence in sensitive domains. Our research demonstrates that sophisticated privacy mechanisms can provide strong mathematical guarantees while maintaining the utility necessary for practical AI applications."}),(0,i.jsx)("p",{className:"body-text text-research-text-secondary",children:"Future research directions include developing more efficient privacy-preserving algorithms, creating standardized privacy evaluation frameworks, and investigating the intersection of privacy preservation with emerging AI paradigms such as large language models and multimodal systems."})]}),(0,i.jsx)(n.P.div,{initial:{opacity:0,y:20},whileInView:{opacity:1,y:0},transition:{duration:.8},viewport:{once:!0},className:"border-t border-accent-ai-purple/20 pt-8",children:(0,i.jsxs)("div",{className:"flex justify-between items-center",children:[(0,i.jsxs)(x(),{href:"/research/language-code-interoperability",className:"inline-flex items-center px-6 py-3 bg-white/10 text-research-text font-medium rounded-2xl border border-accent-ai-purple/20 hover:border-accent-ai-purple/40 backdrop-blur-sm transition-all duration-300",children:[(0,i.jsx)(s.A,{className:"h-4 w-4 mr-2"}),"Previous Article"]}),(0,i.jsxs)(x(),{href:"/research/benchmarking-open-vs-closed-ai",className:"inline-flex items-center px-6 py-3 bg-gradient-to-r from-accent-ai-purple to-accent-lab-purple text-white font-medium rounded-2xl shadow-ai-glow hover:shadow-hero-glow transition-all duration-300",children:["Next Article",(0,i.jsx)(h.A,{className:"h-4 w-4 ml-2"})]})]})})]})})})]})}},32919:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});let i=(0,t(19946).A)("lock",[["rect",{width:"18",height:"11",x:"3",y:"11",rx:"2",ry:"2",key:"1w4ew1"}],["path",{d:"M7 11V7a5 5 0 0 1 10 0v4",key:"fwvmzm"}]])},33109:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});let i=(0,t(19946).A)("trending-up",[["path",{d:"M16 7h6v6",key:"box55l"}],["path",{d:"m22 7-8.5 8.5-5-5L2 17",key:"1t1m79"}]])},42105:(e,a,t)=>{Promise.resolve().then(t.bind(t,18078))},43332:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});let i=(0,t(19946).A)("tag",[["path",{d:"M12.586 2.586A2 2 0 0 0 11.172 2H4a2 2 0 0 0-2 2v7.172a2 2 0 0 0 .586 1.414l8.704 8.704a2.426 2.426 0 0 0 3.42 0l6.58-6.58a2.426 2.426 0 0 0 0-3.42z",key:"vktsd0"}],["circle",{cx:"7.5",cy:"7.5",r:".5",fill:"currentColor",key:"kqv944"}]])},66516:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});let i=(0,t(19946).A)("share-2",[["circle",{cx:"18",cy:"5",r:"3",key:"gq8acd"}],["circle",{cx:"6",cy:"12",r:"3",key:"w7nqdw"}],["circle",{cx:"18",cy:"19",r:"3",key:"1xt0gg"}],["line",{x1:"8.59",x2:"15.42",y1:"13.51",y2:"17.49",key:"47mynk"}],["line",{x1:"15.41",x2:"8.59",y1:"6.51",y2:"10.49",key:"1n3mei"}]])},75525:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});let i=(0,t(19946).A)("shield",[["path",{d:"M20 13c0 5-3.5 7.5-7.66 8.95a1 1 0 0 1-.67-.01C7.5 20.5 4 18 4 13V6a1 1 0 0 1 1-1c2 0 4.5-1.2 6.24-2.72a1.17 1.17 0 0 1 1.52 0C14.51 3.81 17 5 19 5a1 1 0 0 1 1 1z",key:"oel41y"}]])},92657:(e,a,t)=>{"use strict";t.d(a,{A:()=>i});let i=(0,t(19946).A)("eye",[["path",{d:"M2.062 12.348a1 1 0 0 1 0-.696 10.75 10.75 0 0 1 19.876 0 1 1 0 0 1 0 .696 10.75 10.75 0 0 1-19.876 0",key:"1nclc0"}],["circle",{cx:"12",cy:"12",r:"3",key:"1v7zrd"}]])}},e=>{e.O(0,[9066,2018,5647,5525,6874,272,8579,2027,8096,420,8441,5964,7358],()=>e(e.s=42105)),_N_E=e.O()}]);